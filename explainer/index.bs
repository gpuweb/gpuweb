<pre class='metadata'>
Title: WebGPU Explainer
Shortname: webgpu-explainer
Level: 1
Status: LS
Group: webgpu
URL: https://gpuweb.github.io/gpuweb/explainer/
Issue Tracking: gpuweb/gpuweb#1321 https://github.com/gpuweb/gpuweb/issues/1321
No Editor: true
No Abstract: true
Markup Shorthands: markdown yes
Markup Shorthands: dfn yes
Markup Shorthands: idl yes
Markup Shorthands: css no
Assume Explicit For: yes
Boilerplate: repository-issue-tracking no
</pre>

Issue(tabatkins/bikeshed#2006): Set up cross-linking into the WebGPU and WGSL specs.


# Motivation # {#motivation}

See [Introduction](https://gpuweb.github.io/gpuweb/#introduction).


# Security/Privacy # {#security}

See [Malicious use considerations](https://gpuweb.github.io/gpuweb/#malicious-use).


# Additional Background # {#background}


## Sandboxed GPU Processes in Web Browsers ## {#gpu-process}

A major design constraint for WebGPU is that it must be implementable and efficient in browsers that use a GPU-process architecture.
GPU drivers need access to additional kernel syscalls than what's otherwise used for Web content, and many GPU drivers are prone to hangs or crashes.
To improve stability and sandboxing, browsers use a special process that contains the GPU driver and talks with the rest of the browser through asynchronous IPC.
GPU processes are (or will be) used in Chromium, Gecko, and WebKit.

GPU processes are less sandboxed than content processes, and they are typically shared between multiple origins.
Therefore, they must validate all messages, for example to prevent a compromised content process from being able to look at the GPU memory used by another content process.
Most of WebGPU's validation rules are necessary to ensure it is secure to use, so all the validation needs to happen in the GPU process.

Likewise, all GPU driver objects only live in the GPU process, including large allocations (like buffers and textures) and complex objects (like pipelines).
In the content process, WebGPU types (`GPUBuffer`, `GPUTexture`, `GPURenderPipeline`, ...) are mostly just "handles" that identify objects that live in the GPU process.
This means that the CPU and GPU memory used by WebGPU object isn't necessarily known in the content process.
A `GPUBuffer` object can use maybe 150 bytes of CPU memory in the content process but hold a 1GB allocation of GPU memory.

See also the description of [the content and device timelines in the specification](https://gpuweb.github.io/gpuweb/#programming-model-timelines).


# JavaScript API # {#api}


## Adapter Selection and Device Init ## {#initialization}


## Object Validity and Destroyed-ness ## {#invalid-and-destroyed}

### WebGPU's Error Monad ### {#error-monad}

A.k.a. Contagious Internal Nullability.
A.k.a. transparent [promise pipelining](http://erights.org/elib/distrib/pipeline.html).

WebGPU is a very chatty API, with some applications making tens of thousands of calls per frame to render complex scenes.
We have seen that the GPU processes needs to validate the commands to satisfy their security property.
To avoid the overhead of validating commands twice in both the GPU and content process, WebGPU is designed so Javascript calls can be forwarded directly to the GPU process and validated there.
See the error section for more details on what's validated where and how errors are reported.

At the same time, during a single frame WebGPU objects can be created that depend on one another.
For example a `GPUCommandBuffer` can be recorded with commands that use temporary `GPUBuffer`s created in the same frame.
In this example, because of the performance constraint of WebGPU, it is not possible to send the message to create the `GPUBuffer` to the GPU process and synchronously wait for its processing before continuing Javascript execution.

Instead, in WebGPU all objects (like `GPUBuffer`) are created immediately on the content timeline and returned to JavaScript.
The validation is almost all done asynchronously on the "device timeline".
In the good case, when no errors occur (validation or out-of-memory), everything looks to JS as if it is synchronous.
However, when an error occurs in a call, it becomes a no-op (aside from error reporting).
If the call returns an object (like `createBuffer`), the object is tagged as "invalid" on the GPU process side.

All WebGPU calls validate that all their arguments are valid objects.
As a result, if a call takes one WebGPU object and returns a new one, the new object is also invalid (hence the term "contagious").

<figure>
    <figcaption>
        Timeline diagram of messages passing between processes, demonstrating how errors are propagated without synchronization.
    </figcaption>
    <object type="image/svg+xml" data="img/error_monad_timeline_diagram.svg"></object>
</figure>

<div class=example>
    Using the API when doing only valid calls looks like a synchronous API:

    <pre highlight="js">
        const srcBuffer = device.createBuffer({
            size: 4,
            usage: GPUBufferUsage.COPY_SRC
        });

        const dstBuffer = ...;

        const encoder = device.createCommandEncoder();
        encoder.copyBufferToBuffer(srcBuffer, 0, dstBuffer, 0, 4);

        const commands = encoder.finish();
        device.queue.submit([commands]);
    </pre>
</div>

<div class=example>
    Errors propagate contagiously when creating objects:

    <pre highlight="js">
        // The size of the buffer is too big, this causes an OOM and srcBuffer is invalid.
        const srcBuffer = device.createBuffer({
            size: BIG_NUMBER,
            usage: GPUBufferUsage.COPY_SRC
        });

        const dstBuffer = ...;

        // The encoder starts as a valid object.
        const encoder = device.createCommandEncoder();
        // Special case: an invalid object is used when encoding commands so the encoder
        // becomes invalid.
        encoder.copyBufferToBuffer(srcBuffer, 0, dstBuffer, 0, 4);

        // commands, the this argument to GPUCommandEncoder.finish is invalid
        // so the call returns an invalid object.
        const commands = encoder.finish();
        // The command references an invalid object so it becomes a noop.
        device.queue.submit([commands]);
    </pre>
</div>

#### Mental Models #### {#error-monad-mental-model}

One way to interpret WebGPU's semantics is that every WebGPU object is actually a `Promise` internally and that all WebGPU methods are `async` and `await` before using each of the WebGPU objects it gets as argument.
However the execution of the async code is outsourced to the GPU process (where it is actually done synchronously).

Another way, closer to actual implementation details, is to imagine that each `GPUFoo` JS object maps to a `gpu::InternalFoo` C++/Rust object on the GPU process that contains a `bool isValid`.
Then during the validation of each command on the GPU process, the `isValid` are all checked and a new, invalid object is returned if validation fails.
On the content process side, the `GPUFoo` implementation doesn't know if the object is valid or not.

### Early Destruction of WebGPU Objects ### {#early-destroy}

Most of the memory usage of WebGPU objects is in the GPU process: it can be GPU memory held by objects like `GPUBuffer` and `GPUTexture`, serialized commands held in CPU memory by `GPURenderBundles`, or complex object graphs for the WGSL AST in `GPUShaderModule`.
The JavaScript garbage collector (GC) is in the renderer process and doesn't know about the memory usage in the GPU process.
Browsers have many heuristics to trigger GCs but a common one is that it should be triggered on memory pressure scenarios.
However a single WebGPU object can hold on to MBs or GBs of memory without the GC knowing and never trigger the memory pressure event.

It is important for WebGPU applications to be able to directly free the memory used by some WebGPU objects without waiting for the GC.
For example applications might create temporary textures and buffers each frame and without the explicit `.destroy()` call they would quickly run out of GPU memory.
That's why WebGPU has a `.destroy()` method on those object types which can hold on to arbitrary amount of memory.
It signals that the application doesn't need the content of the object anymore and that it can be freed as soon as possible.
Of course, it becomes a validation to use the object after the call to `.destroy()`.

<div class=example>
    <pre highlight="js">
        const dstBuffer = device.createBuffer({
            size: 4
            usage: GPUBufferUsage.COPY_DST
        });

        // The buffer is not destroyed (and valid), success!
        device.queue.writeBuffer(dstBuffer, 0, myData);

        buffer.destroy();

        // The buffer is now destroyed, commands using that would use its
        // content produce validation errors.
        device.queue.writeBuffer(dstBuffer, 0, myData);
    </pre>
</div>

Note that, while this looks somewhat similar to the behavior of an invalid buffer, it is distinct.
Unlike invalidity, destroyed-ness can change after creation, is not contagious, and is validated only when work is actually submitted (e.g. `queue.writeBuffer()` or `queue.submit()`), not when creating dependent objects (like command encoders, see above).


## Errors ## {#errors}

In a simple world, error handling in apps would be synchronous with JavaScript exceptions.
However, for multi-process WebGPU implementations, this is prohibitively expensive.

See [[#invalid-and-destroyed]], which also explains how the *browser* handles errors.

### Problems and Solutions ### {#errors-solutions}

Developers and applications need error handling for a number of cases:

- *Debugging*:
    Getting errors synchronously during development, to break in to the debugger.
- *Fatal Errors*:
    Handling device/adapter loss, either by restoring WebGPU or by fallback to non-WebGPU content.
- *Fallible Allocation*:
    Making fallible GPU-memory resource allocations (detecting out-of-memory conditions).
- *Fallible Validation*:
    Checking success of WebGPU calls, for applications' unit/integration testing, WebGPU
    conformance testing, or detecting errors in data-driven applications (e.g. loading glTF
    models that may exceed device limits).
- *Telemetry*:
    Collecting error logs in deployment, for bug reporting and telemetry.

The following sections go into more details on these cases and how they are solved.

#### Debugging #### {#errors-cases-debugging}

**Solution:** Dev Tools.

Implementations should provide a way to enable synchronous validation,
for example via a "break on WebGPU error" option in the developer tools.

This can be achieved with a content-process-gpu-process round-trip in every validated WebGPU
call, though in practice this would be very slow.
It can be optimized by running a second, approximated mirror of the validation steps in the
content process (it will not always have the same results since it cannot immediately know about
out-of-memory errors).

#### Fatal Errors: Adapter and Device Loss #### {#errors-cases-fatalerrors}

**Solution:** [[#device-loss]].

#### Fallible Allocation, Fallible Validation, and Telemetry #### {#errors-cases-other}

**Solution:** *Error Scopes*.

For important context, see [[#invalid-and-destroyed]]. In particular, all errors (validation and
out-of-memory) are detected asynchronously, in a remote process.
In the WebGPU spec, we refer to the thread of work for each WebGPU device as its "device timeline".

As such, applications need a way to instruct the device timeline on what to do with any errors
that occur. To solve this, WebGPU uses *Error Scopes*.

### Error Scopes ### {#errors-errorscopes}

WebGL exposed errors using a `getError` function returning the first error the last `getError` call.
This is simple, but has two problems.

- It is synchronous, incurring a round-trip and requiring all previously issued work to be finished.
    We solve this by returning errors asynchronously.
- Its flat state model composes poorly: errors can leak to/from unrelated code, possibly in
    libraries/middleware, browser extensions, etc. We solve this with a stack of error "scopes",
    allowing each component to hermetically capture and handle its own errors.

Each device<sup>1</sup> maintains a persistent "error scope" stack state.
Initially, the device's error scope stack is empty.
`GPUDevice.pushErrorScope('validation')` or `GPUDevice.pushErrorScope('out-of-memory')`
begins an error scope and pushes it onto the stack.
This scope captures only errors of a particular type depending on the type of error the application
wants to detect.

`GPUDevice.popErrorScope()` ends an error scope, popping it from the stack and returning a
`Promise<GPUError?>`, which resolves once all enclosed fallible operations have completed and
reported back.
It resolves to `null` if no errors were captured, and otherwise resolves to an object describing
the first error that was captured by the scope - either a `GPUValidationError` or a
`GPUOutOfMemoryError`.

Any device-timeline error from an operation is passed to the top-most error scope on the stack at
the time it was issued.

- If an error scope captures an error, the error is not passed down the stack.
    Each error scope stores only the **first** error it captures; any further errors it captures
    are **silently ignored**.
- If not, the error is passed down the stack to the enclosing error scope.
- If an error reaches the bottom of the stack, it **may**<sup>2</sup> fire the `uncapturederror`
    event on `GPUDevice`<sup>3</sup> (and could issue a console warning as well).

<sup>1</sup>
In the plan to add [[#multi-threading]], error scope state to actually be **per-device, per-realm**.
That is, when a GPUDevice is posted to a Worker for the first time, the error scope stack for
that device+realm is always empty.
(If a GPUDevice is copied *back* to an execution context it already existed on, it shares its
error scope state with all other copies on that execution context.)

<sup>2</sup>
The implementation may not choose to always fire the event for a given error, for example if it
has fired too many times, too many times rapidly, or with too many errors of the same kind.
This is similar to how Dev Tools console warnings work today for WebGL.
In poorly-formed applications, this mechanism can prevent the events from having a significant
performance impact on the system.

<sup>3</sup>
More specifically, with [[#multi-threading]], this event would only exists on the *originating*
`GPUDevice` (the one that came from `createDevice`).
It doesn't exist on `GPUDevice`s produced by sending messages.

```webidl
enum GPUErrorFilter {
    "out-of-memory",
    "validation"
};

interface GPUOutOfMemoryError {
    constructor();
};

interface GPUValidationError {
    constructor(DOMString message);
    readonly attribute DOMString message;
};

typedef (GPUOutOfMemoryError or GPUValidationError) GPUError;

partial interface GPUDevice {
    undefined pushErrorScope(GPUErrorFilter filter);
    Promise<GPUError?> popErrorScope();
};
```

#### How this solves *Fallible Allocation* #### {#errors-errorscopes-allocation}

If a call that fallibly allocates GPU memory (e.g. `createBuffer` or `createTexture`) fails, the
resulting object is invalid (same as if there were a validation error), but an `'out-of-memory'`
error is generated.
An `'out-of-memory'` error scope can be used to detect it.

**Example: tryCreateBuffer**

```ts
async function tryCreateBuffer(device: GPUDevice, descriptor: GPUBufferDescriptor): Promise<GPUBuffer | null> {
  device.pushErrorScope('out-of-memory');
  const buffer = device.createBuffer(descriptor);
  if (await device.popErrorScope() !== null) {
    return null;
  }
  return buffer;
}
```

This interacts with buffer mapping in subtle ways, but they are not explained here.
The principle used to design the interaction is that app code should need to handle as few
different edge cases as possible, so multiple kinds of situations should result in the same
behavior.

#### How this solves *Fallible Validation* #### {#errors-errorscopes-validation}

A `'validation'` error scope can be used to detect validation errors, as above.

**Example: Testing**

```ts
device.pushErrorScope('out-of-memory');
device.pushErrorScope('validation');

{
  // (Do stuff that shouldn't produce errors.)

  {
    device.pushErrorScope('validation');
    device.doOperationThatIsExpectedToError();
    device.popErrorScope().then(error => { assert(error !== null); });
  }

  // (More stuff that shouldn't produce errors.)
}

// Detect unexpected errors.
device.popErrorScope().then(error => { assert(error === null); });
device.popErrorScope().then(error => { assert(error === null); });
```

#### How this solves *Telemetry* #### {#errors-errorscopes-telemetry}

As mentioned above, if an error is not captured by an error scope, it **may** fire the
originating device's `uncapturederror` event.
Applications can either watch for that event, or encapsulate parts of their application with
error scopes, to detect errors for generating error reports.

`uncapturederror` is not strictly necessary to solve this, but has the benefit of providing a
single stream for uncaptured errors from all threads.

#### Error Messages and Debug Labels #### {#errors-errorscopes-labels}

Every WebGPU object has a read-write attribute, `label`, which can be set by the application to
provide information for debugging tools (error messages, native profilers like Xcode, etc.)
Every WebGPU object creation descriptor has a member `label` which sets the initial value of the
attribute.

For both debugging (dev tools messages) and telemetry, implementations can choose to report some
kind of "stack trace" in their error messages, taking advantage of object debug labels.
For example:

```
<myQueue>.submit failed:
- commands[0] (<mainColorPass>) was invalid:
- in setIndexBuffer, indexBuffer (<mesh3.indices>) was invalid:
- in createBuffer, desc.usage was invalid (0x89)
```


## Adapter and Device Loss ## {#device-loss}

Any situation that prevents further use of a `GPUDevice`, regardless of whether it is caused by a
WebGPU call (e.g. `device.destroy()`, unrecoverable out-of-memory, GPU process crash, or GPU
reset) or happens externally (e.g. GPU unplugged), results in a device loss.

**Design principle:**
There should be as few different-looking error behaviors as possible.
This makes it easier for developers to test their app's behavior in different situations,
improves robustness of applications in the wild, and improves portability between browsers.

Issue: Finish this explainer (see [ErrorHandling.md](https://github.com/gpuweb/gpuweb/blob/main/design/ErrorHandling.md#fatal-errors-requestadapter-requestdevice-and-devicelost)).


## Buffer Mapping ## {#buffer-mapping}

A `GPUBuffer` represents a memory allocations usable by other GPU operations.
This memory can be accessed linearly, contrary to `GPUTexture` for which the actual memory layout of sequences of texels are unknown. Think of `GPUBuffers` as the result of `gpu_malloc()`.

**CPU&rarr;GPU:** When using WebGPU, applications need to transfer data from JavaScript to `GPUBuffer` very often and potentially in large quantities.
This includes mesh data, drawing and computations parameters, ML model inputs, etc.
That's why an efficient way to update `GPUBuffer` data is needed. `GPUQueue.writeBuffer` is reasonably efficient but includes at least an extra copy compared to the buffer mapping used for writing buffers.

**GPU&rarr;CPU:** Applications also often need to transfer data from the GPU to Javascript, though usually less often and in lesser quantities.
This includes screenshots, statistics from computations, simulation or ML model results, etc.
This transfer is done with buffer mapping for reading buffers.

### Background: Memory Visibility with GPUs and GPU Processes ### {#buffer-mapping-background}

The two major types of GPUs are called "integrated GPUs" and "discrete GPUs".
Discrete GPUs are separate from the CPU; they usually come as PCI-e cards that you plug into the motherboard of a computer.
Integrated GPUs live on the same die as the CPU and don't have their own memory chips; instead, they use the same RAM as the CPU.

When using a discrete GPU, it's easy to see that most GPU memory allocations aren't visible to the CPU because they are inside the GPU's RAM (or VRAM for Video RAM).
For integrated GPUs most memory allocations are in the same physical places, but not made visible to the GPU for various reasons (for example, the CPU and GPU can have separate caches for the same memory, so accesses are not cache-coherent).
Instead, for the CPU to see the content of a GPU buffer, it must be "mapped", making it available in the virtual memory space of the application (think of mapped as in `mmap()`).
GPUBuffers must be specially allocated in order to be mappable - this can make it less efficient to access from the GPU (for example if it needs to be allocate in RAM instead of VRAM).

All this discussion was centered around native GPU APIs, but in browsers, the GPU driver is loaded in the _GPU process_, so native GPU buffers can be mapped only in the GPU process's virtual memory.
In general, it is not possible to map the buffer directly inside the _content process_ (though some systems can do this, providing optional optimizations).
To work with this architecture an extra "staging" allocation is needed in shared memory between the GPU process and the content process.

The table below recapitulates which type of memory is visible where:

<table class="data">
    <thead>
        <tr>
            <th>
            <th> Regular `ArrayBuffer`
            <th> Shared Memory
            <th> Mappable GPU buffer
            <th> Non-mappable GPU buffer (or texture)
        </tr>
    </thead>
    <tr>
        <td> CPU, in the content process
        <td> **Visible**
        <td> **Visible**
        <td> Not visible
        <td> Not visible
    <tr>
        <td> CPU, in the GPU process
        <td> Not visible
        <td> **Visible**
        <td> **Visible**
        <td> Not visible
    <tr>
        <td> GPU
        <td> Not visible
        <td> Not visible
        <td> **Visible**
        <td> **Visible**
</table>

### CPU-GPU Ownership Transfer ### {#buffer-mapping-ownership}

In native GPU APIs, when a buffer is mapped, its content becomes accessible to the CPU.
At the same time the GPU can keep using the buffer's content, which can lead to data races between the CPU and the GPU.
This means that the usage of mapped buffer is simple but leaves the synchronization to the application.

On the contrary, WebGPU prevents almost all data races in the interest of portability and consistency.
In WebGPU there is even more risk of non-portability with races on mapped buffers because of the additional "shared memory" step that may be necessary on some drivers.
That's why `GPUBuffer` mapping is done as an ownership transfer between the CPU and the GPU.
At each instant, only one of the two can access it, so no race is possible.

When an application requests to map a buffer, it initiates a transfer of the buffer's ownership to the CPU.
At this time, the GPU may still need to finish executing some operations that use the buffer, so the transfer doesn't complete until all previously-enqueued GPU operations are finished.
That's why mapping a buffer is an asynchronous operation (we'll discuss the other arguments below):

<xmp highlight=idl>
typedef [EnforceRange] unsigned long GPUMapModeFlags;
interface GPUMapMode {
    const GPUFlagsConstant READ  = 0x0001;
    const GPUFlagsConstant WRITE = 0x0002;
};

partial interface GPUBuffer {
  Promise<undefined> mapAsync(GPUMapModeFlags mode,
                              optional GPUSize64 offset = 0,
                              optional GPUSize64 size);
};
</xmp>

<div class=example>
    Using it is done like so:

    <pre highlight="js">
        // Mapping a buffer for writing. Here offset and size are defaulted t
        // so the whole buffer is mapped.
        const myMapWriteBuffer = ...;
        await myMapWriteBuffer.mapAsync(GPUMapMode.WRITE);

        // Mapping a buffer for reading. Only the first four bytes are mapped.
        const myMapReadBuffer = ...;
        await myMapReadBuffer.mapAsync(GPUMapMode.READ, 0, 4);
    </pre>
</div>

Once the application has finished using the buffer on the CPU, it can transfer ownership back to the GPU by unmapping it.
This is an immediate operation that makes the application lose all access to the buffer on the CPU (i.e. detaches `ArrayBuffers`):

<xmp highlight=idl>
partial interface GPUBuffer {
  undefined unmap();
};
</xmp>

<div class=example>
    Using it is done like so:

    <pre highlight="js">
        const myMapReadBuffer = ...;
        await myMapReadBuffer.mapAsync(GPUMapMode.READ, 0, 4);
        // Do something with the mapped buffer.
        buffer.unmap();
    </pre>
</div>

When transferring ownership to the CPU, a copy may be necessary from the underlying mapped buffer to shared memory visible to the content process.
To avoid copying more than necessary, the application can specify which range it is interested in when calling `GPUBuffer.mapAsync`.

`GPUBuffer.mapAsync`'s `mode` argument controls which type of mapping operation is performed.
At the moment its values are redundant with the buffer creation's usage flags, but it is present for explicitness and future extensibility.

While a `GPUBuffer` is owned by the CPU, it is not possible to submit any operations on the device timeline that use it; otherwise, a validation error is produced.
However it is valid (and encouraged!) to record `GPUCommandBuffer`s using the `GPUBuffer`.

### Creation of Mappable Buffers ### {#buffer-mapping-creation}

The physical memory location for a `GPUBuffer`'s underlying buffer depends on whether it should be mappable and whether it is mappable for reading or writing (native APIs give some control on the CPU cache behavior for example).
At the moment mappable buffers can only be used to transfer data (so they can only have the correct `COPY_SRC` or `COPY_DST` usage in addition to a `MAP_*` usage),
That's why applications must specify that buffers are mappable when they are created using the (currently) mutually exclusive `GPUBufferUsage.MAP_READ` and `GPUBufferUsage.MAP_WRITE` flags:

<div class=example>
    <pre highlight="js">
        const myMapReadBuffer = device.createBuffer({
            usage: GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST,
            size: 1000,
        });
        const myMapWriteBuffer = device.createBuffer({
            usage: GPUBufferUsage.MAP_WRITE | GPUBufferUsage.COPY_SRC,
            size: 1000,
        });
    </pre>
</div>

### Accessing Mapped Buffers ### {#buffer-mapping-access}

Once a `GPUBuffer` is mapped, it is possible to access its memory from JavaScript
 This is done by calling `GPUBuffer.getMappedRange`, which returns an `ArrayBuffer` called a "mapping".
These are available until `GPUBuffer.unmap` or `GPUBuffer.destroy` is called, at which point they are detached.
These `ArrayBuffer`s typically aren't new allocations, but instead pointers to some kind of shared memory visible to the content process (IPC shared memory, `mmap`ped file descriptor, etc.)

When transferring ownership to the GPU, a copy may be necessary from the shared memory to the underlying mapped buffer.
`GPUBuffer.getMappedRange` takes an optional range of the buffer to map (for which `offset` 0 is the start of the buffer).
This way the browser knows which parts of the underlying `GPUBuffer` have been "invalidated" and need to be updated from the memory mapping.

The range must be within the range requested in `mapAsync()`.

<xmp highlight=idl>
partial interface GPUBuffer {
  ArrayBuffer getMappedRange(optional GPUSize64 offset = 0,
                             optional GPUSize64 size);
};
</xmp>

<div class=example>
    Using it is done like so:

    <pre highlight="js">
        const myMapReadBuffer = ...;
        await myMapReadBuffer.mapAsync(GPUMapMode.READ);
        const data = myMapReadBuffer.getMappedRange();
        // Do something with the data
        myMapReadBuffer.unmap();
    </pre>
</div>

### Mapping Buffers at Creation ### {#buffer-mapping-at-creation}

A common need is to create a `GPUBuffer` that is already filled with some data.
This could be achieved by creating a final buffer, then a mappable buffer, filling the mappable buffer, and then copying from the mappable to the final buffer, but this would be inefficient.
Instead this can be done by making the buffer CPU-owned at creation: we call this "mapped at creation".
All buffers can be mapped at creation, even if they don't have the `MAP_WRITE` buffer usages.
The browser will just handle the transfer of data into the buffer for the application.

Once a buffer is mapped at creation, it behaves as regularly mapped buffer: `GPUBUffer.getMappedRange()` is used to retrieve `ArrayBuffer`s, and ownership is transferred to the GPU with `GPUBuffer.unmap()`.

<div class=example>
    Mapping at creation is done by passing `mappedAtCreation: true` in the buffer descriptor on creation:

    <pre highlight="js">
        const buffer = device.createBuffer({
            usage: GPUBufferUsage.UNIFORM,
            size: 256,
            mappedAtCreation: true,
        });
        const data = buffer.getMappedRange();
        // write to data
        buffer.unmap();
    </pre>
</div>

When using advanced methods to transfer data to the GPU (with a rolling list of buffers that are mapped or being mapped), mapping buffer at creation can be used to immediately create additional space where to put data to be transferred.

### Examples ### {#buffer-mapping-examples}

<div class=example>
    The optimal way to create a buffer with initial data, for example here a [Draco](https://google.github.io/draco/)-compressed 3D mesh:

    <pre highlight="js">
        const dracoDecoder = ...;

        const buffer = device.createBuffer({
            usage: GPUBuffer.VERTEX | GPUBuffer.INDEX,
            size: dracoDecoder.decompressedSize,
            mappedAtCreation: true,
        });

        dracoDecoder.decodeIn(buffer.getMappedRange());
        buffer.unmap();
    </pre>
</div>

<div class=example>
    Retrieving data from a texture rendered on the GPU:

    <pre highlight="js">
        const texture = getTheRenderedTexture();

        const readbackBuffer = device.createBuffer({
            usage: GPUBufferUsage.COPY_DST | GPUBufferUsage.MAP_READ,
            size: 4 * textureWidth * textureHeight,
        });

        // Copy data from the texture to the buffer.
        const encoder = device.createCommandEncoder();
        encoder.copyTextureToBuffer(
            { texture },
            { buffer, rowPitch: textureWidth * 4 },
            [textureWidth, textureHeight],
        );
        device.submit([encoder.finish()]);

        // Get the data on the CPU.
        await buffer.mapAsync(GPUMapMode.READ);
        saveScreenshot(buffer.getMappedRange());
        buffer.unmap();
    </pre>
</div>

<div class=example>
    Updating a bunch of data on the GPU for a frame:

    <pre highlight="js">
        void frame() {
            // Create a new buffer for our updates. In practice we would
            // reuse buffers from frame to frame by re-mapping them.
            const stagingBuffer = device.createBuffer({
                usage: GPUBufferUsage.MAP_WRITE | GPUBufferUsage.COPY_SRC,
                size: 16 * objectCount,
                mappedAtCreation: true,
            });
            const stagingData = new Float32Array(stagingBuffer.getMappedRange());

            // For each draw we are going to:
            //  - Put the data for the draw in stagingData.
            //  - Record a copy from the stagingData to the uniform buffer for the draw
            //  - Encoder the draw
            const copyEncoder = device.createCommandEncoder();
            const drawEncoder = device.createCommandEncoder();
            const renderPass = myCreateRenderPass(drawEncoder);
            for (var i = 0; i < objectCount; i++) {
                stagingData[i * 4 + 0] = ...;
                stagingData[i * 4 + 1] = ...;
                stagingData[i * 4 + 2] = ...;
                stagingData[i * 4 + 3] = ...;

                const {uniformBuffer, uniformOffset} = getUniformsForDraw(i);
                copyEncoder.copyBufferToBuffer(
                    stagingData, i * 16,
                    uniformBuffer, uniformOffset,
                    16);

                encodeDraw(renderPass, {uniformBuffer, uniformOffset});
            }
            renderPass.endPass();

            // We are finished filling the staging buffer, unmap() it so
            // we can submit commands that use it.
            stagingBuffer.unmap();

            // Submit all the copies and then all the draws. The copies
            // will happen before the draw such that each draw will use
            // the data that was filled inside the for-loop above.
            device.queue.submit([
                copyEncoder.finish(),
                drawEncoder.finish()
            ]);
        }
    </pre>
</div>

## Multi-Threading ## {#multi-threading}


## Command Encoding and Submission ## {#command-encoding}


## Pipelines ## {#pipelines}


## Image, Video, and Canvas input ## {#image-input}


## Canvas Output ## {#canvas-output}


## Bitflags ## {#bitflags}

WebGPU uses C-style bitflags in several places.
(Search `GPUFlagsConstant` in the spec for instances.)
A typical bitflag definition looks like this:

<xmp highlight=idl>
typedef [EnforceRange] unsigned long GPUColorWriteFlags;
[Exposed=Window]
interface GPUColorWrite {
    const GPUFlagsConstant RED   = 0x1;
    const GPUFlagsConstant GREEN = 0x2;
    const GPUFlagsConstant BLUE  = 0x4;
    const GPUFlagsConstant ALPHA = 0x8;
    const GPUFlagsConstant ALL   = 0xF;
};
</xmp>

This was chosen because there is no other particularly ergonomic way to describe
"enum sets" in JavaScript today.

Bitflags are used in WebGL, which many WebGPU developers will be familiar with.
They also match closely with the API shape that would be used by many native-language bindings.

The closest option is `sequence<enum type>`, but it doesn't naturally describe
an unordered set of unique items and doesn't easily allow things like
`GPUColorWrite.ALL` above.
Additionally, `sequence<enum type>` has significant overhead, so we would have to avoid it in any
APIs that are expected to be "hot paths" (like command encoder methods), causing inconsistency with
parts of the API that *do* use it.

See also issue [#747](https://github.com/gpuweb/gpuweb/issues/747)
which mentions that strongly-typed bitflags in JavaScript would be useful.


# WebGPU Shading Language # {#wgsl}
